from keras.datasets import imdb
from keras.preprocessing import sequence 
import tensorflow as tf
import os 
import numpy as np
VOCABULARY_SIZE=88584
MAXLEN=250
BATCH_SIZE=64
(train_data,train_labels),(test_data,test_labels)=imdb.load_data(num_words=VOCABULARY_SIZE)
train_data=sequence.pad_sequences(train_data,MAXLEN)
test_data=sequence.pad_sequences(test_data,MAXLEN)
model=tf.keras.Sequential([
    tf.keras.layers.Embedding(VOCABULARY_SIZE,32),
    tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(1,activation='sigmoid')
])
model.summary()
model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['acc'])
history=model.fit(train_data,train_labels,epochs=10,validation_split=0.2)
results=model.evaluate(test_data,test_labels)
print(results)
from tensorflow import keras
word_index=imdb.get_word_index()
def encode_text(text):
  tokens=keras.preprocessing.text.text_to_word_sequence(text)
  tokens=[word_index[word] if word in word_index else 0 for word in tokens]
  return sequence.pad_sequences([tokens],MAXLEN)[0]

text='the movie was suoppa amazing, so amazing '
encoded = encode_text(text)
print(encoded)
reverse_word_index={value:key for (key,value) in word_index.items()}
def decode_integers(tint):
   text=""
   for num in tint:
     if num!=0:
      text+=reverse_word_index[num]+' '
   
   return text[:-1]

print(decode_integers(encoded))
def predict(text):
  encoded_text=encode_text(text)
  pred=np.zeros((1,250))
  pred[0]=encoded_text
  result= model.predict(pred)
  print(result[0])

positive_review='love the movie it gave me so much pleasure amazing out of this world'
print(predict(positive_review))
negative_review='the movie sucks i hate it so boring and monotonous  the worst thing i have seen my life'
print(predict(negative_review))
